{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deptinfo\\Anaconda2\\envs\\py36\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.layers import recurrent,Bidirectional\n",
    "import argparse\n",
    "import re\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "from numpy import zeros as np_zeros\n",
    "from keras.layers import Activation, TimeDistributed, Dense, RepeatVector, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.callbacks import ModelCheckpoint, TensorBoard, CSVLogger, LambdaCallback\n",
    "from numpy.random import seed as random_seed\n",
    "from numpy.random import randint as random_randint\n",
    "from numpy.random import choice as random_choice\n",
    "from numpy.random import randint as random_randint\n",
    "from numpy.random import shuffle as random_shuffle\n",
    "from numpy.random import rand\n",
    "import os\n",
    "import pickle\n",
    "import logging\n",
    "from time import time\n",
    "import sys\n",
    "#from kitchen.text.converters import getwriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import errno\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "from hashlib import sha256\n",
    "\n",
    "import re\n",
    "\n",
    "import json\n",
    "\n",
    "import itertools\n",
    "\n",
    "import logging\n",
    "\n",
    "import requests\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATA_FILES_PATH = \"~/Downloads/data\"\n",
    "\n",
    "DATA_FILES_FULL_PATH = os.path.expanduser(DATA_FILES_PATH)\n",
    "\n",
    "DATA_FILES_URL = \"http://www.statmt.org/wmt14/training-monolingual-news-crawl/news.2013.fr.shuffled.gz\"\n",
    "\n",
    "NEWS_FILE_NAME_COMPRESSED = os.path.join(DATA_FILES_FULL_PATH, \"trainfile.txt\") # 1.1 GB\n",
    "\n",
    "NEWS_FILE_NAME_ENGLISH = \"trainfile.txt\"\n",
    "NEWS_FILE_NAME = os.path.join(DATA_FILES_FULL_PATH, NEWS_FILE_NAME_ENGLISH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_the_news_data():\n",
    "\n",
    "    \"\"\"Download the news data\"\"\"\n",
    "\n",
    "    #LOGGER.info(\"Downloading\")\n",
    "\n",
    "    try:\n",
    "\n",
    "        os.makedirs(os.path.dirname(NEWS_FILE_NAME_COMPRESSED))\n",
    "\n",
    "    except OSError as exception:\n",
    "\n",
    "        if exception.errno != errno.EEXIST:\n",
    "\n",
    "            raise\n",
    "\n",
    "    with open(NEWS_FILE_NAME_COMPRESSED, \"wb\") as output_file:\n",
    "\n",
    "        response = requests.get(DATA_FILES_URL, stream=True)\n",
    "\n",
    "        total_length = response.headers.get('content-length')\n",
    "\n",
    "        downloaded = percentage = 0\n",
    "\n",
    "        print(\"»\"*100)\n",
    "\n",
    "        total_length = int(total_length)\n",
    "\n",
    "        for data in response.iter_content(chunk_size=4096):\n",
    "\n",
    "            downloaded += len(data)\n",
    "\n",
    "            output_file.write(data)\n",
    "\n",
    "            new_percentage = 100 * downloaded // total_length\n",
    "\n",
    "            if new_percentage > percentage:\n",
    "\n",
    "                print(\"☑\", end=\"\")\n",
    "\n",
    "                percentage = new_percentage\n",
    "\n",
    "    print()\n",
    "\n",
    "\n",
    "\n",
    "def uncompress_data():\n",
    "\n",
    "    \"\"\"Uncompress the data files\"\"\"\n",
    "\n",
    "    import gzip\n",
    "\n",
    "    with gzip.open(NEWS_FILE_NAME_COMPRESSED, 'rb') as compressed_file:\n",
    "\n",
    "        with open(NEWS_FILE_NAME_COMPRESSED[:-3], 'wb') as outfile:\n",
    "\n",
    "            outfile.write(compressed_file.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download_the_news_data()\n",
    "uncompress_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"]=\"3\"\n",
    "\n",
    "LOGGER = logging.getLogger(__name__) # Every log will use the module name\n",
    "LOGGER.addHandler(logging.StreamHandler())\n",
    "LOGGER.setLevel(logging.DEBUG)\n",
    "\n",
    "DATASET_FILENAME = 'train/trainfile.txt'\n",
    "test_set_fraction=0.1\n",
    "NUMBER_OF_EPOCHS = 30\n",
    "RNN = recurrent.LSTM\n",
    "INPUT_LAYERS = 1\n",
    "OUTPUT_LAYERS = 1\n",
    "AMOUNT_OF_DROPOUT = 0.5\n",
    "BATCH_SIZE = 50\n",
    "SAMPLES_PER_EPOCH = 1000\n",
    "HIDDEN_SIZE = 512\n",
    "INITIALIZATION = \"he_normal\"  # : Gaussian initialization scaled by fan_in (He et al., 2014)\n",
    "NUMBER_OF_CHARS = 100  # 75\n",
    "CHARS = list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ .\")\n",
    "INVERTED = True\n",
    "MODEL_CHECKPOINT_DIRECTORYNAME = 'models'\n",
    "MODEL_CHECKPOINT_FILENAME = 'weights.{epoch:02d}-{val_loss:.2f}.hdf5'\n",
    "MODEL_DATASET_PARAMS_FILENAME = 'dataset_params.pickle'\n",
    "MODEL_STARTING_CHECKPOINT_FILENAME = 'weights.hdf5'\n",
    "CSV_LOG_FILENAME = 'log.csv'\n",
    "# Parameters for the model and dataset\n",
    "MAX_INPUT_LEN = 40\n",
    "MIN_INPUT_LEN = 3\n",
    "AMOUNT_OF_NOISE = 0.2 / MAX_INPUT_LEN\n",
    "NUMBER_OF_CHARS = 100  # 75\n",
    "CHARS = list(\"abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZéè .\")\n",
    "\n",
    "# Some cleanup:\n",
    "NORMALIZE_WHITESPACE_REGEX = re.compile(r'[^\\S\\n]+', re.UNICODE)  # match all whitespace except newlines\n",
    "RE_DASH_FILTER = re.compile(r'[\\-\\˗\\֊\\‐\\‑\\‒\\–\\—\\⁻\\₋\\−\\﹣\\－]', re.UNICODE)\n",
    "RE_APOSTROPHE_FILTER = re.compile(r'&#39;|[ʼ՚＇‘’‛❛❜ߴߵ`‵´ˊˋ{}{}{}{}{}{}{}{}{}]'.format(\n",
    "                                        chr(768), chr(769), chr(832), chr(833), chr(2387), chr(5151),\n",
    "                                        chr(5152), chr(65344), chr(8242)),\n",
    "                                    re.UNICODE)\n",
    "RE_LEFT_PARENTH_FILTER = re.compile(r'[\\(\\[\\{\\⁽\\₍\\❨\\❪\\﹙\\（]', re.UNICODE)\n",
    "RE_RIGHT_PARENTH_FILTER = re.compile(r'[\\)\\]\\}\\⁾\\₎\\❩\\❫\\﹚\\）]', re.UNICODE)\n",
    "ALLOWED_CURRENCIES = \"\"\"¥£₪$€฿₨\"\"\"\n",
    "ALLOWED_PUNCTUATION = \"\"\"-!?/;\"'%&<>.()[]{}@#:,|=*\"\"\"\n",
    "RE_BASIC_CLEANER = re.compile(r'[^\\w\\s{}{}]'.format(\n",
    "                                    re.escape(ALLOWED_CURRENCIES), re.escape(ALLOWED_PUNCTUATION)),\n",
    "                                re.UNICODE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model(output_len, chars=None):\n",
    "    \"\"\"Generate the model\"\"\"\n",
    "    print('Build model...')\n",
    "    chars = chars or CHARS\n",
    "    model = Sequential()\n",
    "    # \"Encode\" the input sequence using an RNN, producing an output of HIDDEN_SIZE\n",
    "    # note: in a situation where your input sequences have a variable length,\n",
    "    # use input_shape=(None, nb_feature).\n",
    "    for layer_number in range(INPUT_LAYERS):\n",
    "        model.add(Bidirectional(RNN(HIDDEN_SIZE, init=INITIALIZATION,\n",
    "                                 return_sequences=True), input_shape=(None, len(chars))))\n",
    "        model.add(Dropout(AMOUNT_OF_DROPOUT))\n",
    "    # For the decoder's input, we repeat the encoded input for each time step\n",
    "   # model.add(RepeatVector(output_len))\n",
    "    # The decoder RNN could be multiple layers stacked or a single layer\n",
    "    for _ in range(OUTPUT_LAYERS):\n",
    "        model.add(Bidirectional(RNN(HIDDEN_SIZE, return_sequences=True, init=INITIALIZATION)))\n",
    "        model.add(Dropout(AMOUNT_OF_DROPOUT))\n",
    "\n",
    "    # For each of step of the output sequence, decide which character should be chosen\n",
    "    model.add(TimeDistributed(Dense(len(chars), init=INITIALIZATION)))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Colors(object):\n",
    "    \"\"\"For nicer printouts\"\"\"\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_samples(model, dataset, epoch, logs, X_dev_batch, y_dev_batch):\n",
    "    \"\"\"Selects 10 samples from the dev set at random so we can visualize errors\"\"\"\n",
    "    #UTF8Writer = getwriter('utf8')\n",
    "    #sys.stdout = UTF8Writer(sys.stdout)\n",
    "    #PYTHONIOENCODING=utf8\n",
    "    for _ in range(10):\n",
    "        ind = random_randint(0, len(X_dev_batch))\n",
    "        row_X, row_y = X_dev_batch[np.array([ind])], y_dev_batch[np.array([ind])]\n",
    "        preds = model.predict_classes(row_X, verbose=0)\n",
    "        q = dataset.character_table.decode(row_X[0])\n",
    "        correct = dataset.character_table.decode(row_y[0])\n",
    "        guess = dataset.character_table.decode(preds[0], calc_argmax=False)\n",
    "\n",
    "        #if INVERTED:\n",
    "         #   print('Q', q[::-1])  # inverted back!\n",
    "        #else:\n",
    "         #   print('Q', q)\n",
    "\n",
    "        #print('A', correct)\n",
    "        #print(Colors.ok + '☑' + Colors.close if correct == guess else Colors.fail + '☒' + Colors.close, guess)\n",
    "        #print('---')\n",
    "        \n",
    "        with open(\"data/outFile.txt\", \"a\", encoding = \"utf-8\") as out:\n",
    "          if INVERTED:\n",
    "            out.write('Q '+ q[::-1])  # inverted back!\n",
    "          else:\n",
    "            out.write('Q '+ q)\n",
    "        with open(\"data/outFile.txt\", \"a\", encoding = \"utf-8\") as out:\n",
    "          out.write('A '+ correct)\n",
    "          if correct == guess:\n",
    "            out.write(Colors.ok + '?' + ' ' + guess)\n",
    "          else:\n",
    "            out.write(Colors.fail + '?' + ' ' + guess)\n",
    "          #out.write(Colors.ok + '☑' + Colors.close if correct == guess else Colors.fail + '☒' + Colors.close, guess)\n",
    "          out.write('---')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def iterate_training(model, dataset, initial_epoch):\n",
    "    \"\"\"Iterative Training\"\"\"\n",
    "\n",
    "    checkpoint = ModelCheckpoint(MODEL_CHECKPOINT_DIRECTORYNAME + '/' + MODEL_CHECKPOINT_FILENAME,\n",
    "                                 save_best_only=True)\n",
    "    tensorboard = TensorBoard()\n",
    "    csv_logger = CSVLogger(CSV_LOG_FILENAME)\n",
    "\n",
    "    X_dev_batch, y_dev_batch = next(dataset.dev_set_batch_generator(1000))\n",
    "    show_samples_callback = LambdaCallback(\n",
    "        on_epoch_end=lambda epoch, logs: show_samples(model, dataset, epoch, logs, X_dev_batch, y_dev_batch))\n",
    "\n",
    "    train_batch_generator = dataset.train_set_batch_generator(BATCH_SIZE)\n",
    "    validation_batch_generator = dataset.dev_set_batch_generator(BATCH_SIZE)\n",
    "\n",
    "    model.fit_generator(train_batch_generator,\n",
    "                        steps_per_epoch=SAMPLES_PER_EPOCH/BATCH_SIZE,\n",
    "                        epochs=NUMBER_OF_EPOCHS,\n",
    "                        validation_data=validation_batch_generator,\n",
    "                        validation_steps=SAMPLES_PER_EPOCH,\n",
    "                        callbacks=[checkpoint, tensorboard, csv_logger, show_samples_callback],\n",
    "                        verbose=1,\n",
    "                        initial_epoch=initial_epoch)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataset_params(dataset):\n",
    "    params = { 'chars': dataset.chars, 'y_max_length': dataset.y_max_length }\n",
    "    with open(MODEL_CHECKPOINT_DIRECTORYNAME + '/' + MODEL_DATASET_PARAMS_FILENAME, 'wb') as f:\n",
    "        pickle.dump(params, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main_news():\n",
    "    \"\"\"Main\"\"\"\n",
    "    checkpoint_filename=None\n",
    "    dataset_params_filename=None\n",
    "    initial_epoch=0\n",
    "    dataset = DataSet(DATASET_FILENAME)\n",
    "\n",
    "    if not os.path.exists(MODEL_CHECKPOINT_DIRECTORYNAME):\n",
    "        os.makedirs(MODEL_CHECKPOINT_DIRECTORYNAME)\n",
    "\n",
    "    if dataset_params_filename is not None:\n",
    "        with open(dataset_params_filename, 'rb') as f:\n",
    "            dataset_params = pickle.load(f)\n",
    "\n",
    "        assert dataset_params['chars'] == dataset.chars\n",
    "        assert dataset_params['y_max_length'] == dataset.y_max_length\n",
    "\n",
    "    else:\n",
    "        save_dataset_params(dataset)\n",
    "\n",
    "    model = generate_model(dataset.y_max_length, dataset.chars)\n",
    "\n",
    "    if checkpoint_filename is not None:\n",
    "        model.load_weights(checkpoint_filename)\n",
    "\n",
    "    iterate_training(model, dataset, initial_epoch)\n",
    "\n",
    "#def read_news():\n",
    "    #UTF8Writer = getwriter('utf8')\n",
    "    #sys.stdout = UTF8Writer(sys.stdout)\n",
    " #   PYTHONIOENCODING=UTF-8\n",
    "  #  print(\"Reading news\")\n",
    "   # news = open(DATASET_FILENAME, encoding='utf-8').read()\n",
    "    #print(\"Read news\")\n",
    "\n",
    "    #lines = [line for line in news.split('\\n')]\n",
    "    #print(\"Read {} lines of input corpus\".format(len(lines)))\n",
    "\n",
    "    #lines = [clean_text(line) for line in lines]\n",
    "    #print(\"Cleaned text\")\n",
    "\n",
    "    #counter = Counter()\n",
    "    #for line in lines:\n",
    "     #   counter += Counter(line)\n",
    "    #most_popular_chars = {key for key, _value in counter.most_common(NUMBER_OF_CHARS)}\n",
    "    #print(\"most popular chars are ready\")\n",
    "    #lines = [line for line in lines if line and not bool(set(line) - most_popular_chars)]\n",
    "    #print(\"Left with {} lines of input corpus\".format(len(lines)))\n",
    "\n",
    "    #return lines\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(object):\n",
    "    \"\"\"\n",
    "    Loads news articles from a file, generates misspellings and vectorizes examples.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_filename, test_set_fraction=0.1, inverted=True):\n",
    "        self.inverted = inverted\n",
    "\n",
    "        news = self.read_news(dataset_filename)\n",
    "        questions, answers = self.generate_examples(news)\n",
    "\n",
    "        chars_answer = set.union(*(set(answer) for answer in answers))\n",
    "        chars_question = set.union(*(set(question) for question in questions))\n",
    "        self.chars = sorted(list(set.union(chars_answer, chars_question)))\n",
    "        self.character_table = CharacterTable(self.chars)\n",
    "\n",
    "        split_at = int(len(questions) * (1 - test_set_fraction))\n",
    "        (self.questions_train, self.questions_dev) = (questions[:split_at], questions[split_at:])\n",
    "        (self.answers_train, self.answers_dev) = (answers[:split_at], answers[split_at:])\n",
    "\n",
    "        self.x_max_length = max(len(question) for question in questions)\n",
    "        self.y_max_length = max(len(answer) for answer in answers)\n",
    "\n",
    "        self.train_set_size = len(self.questions_train)\n",
    "        self.dev_set_size = len(self.questions_dev)\n",
    "\n",
    "        print(\"Completed pre-processing\")\n",
    "\n",
    "    def train_set_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(self.questions_train, self.answers_train, batch_size)\n",
    "\n",
    "    def dev_set_batch_generator(self, batch_size):\n",
    "        return self.batch_generator(self.questions_dev, self.answers_dev, batch_size)\n",
    "\n",
    "    def batch_generator(self, questions, answers, batch_size):\n",
    "        start_index = 0\n",
    "\n",
    "        while True:\n",
    "            questions_batch = []\n",
    "            answers_batch = []\n",
    "\n",
    "            while len(questions_batch) < batch_size:\n",
    "                take = min(len(questions) - start_index, batch_size - len(questions_batch))\n",
    "\n",
    "                questions_batch.extend(questions[start_index: start_index + take])\n",
    "                answers_batch.extend(answers[start_index: start_index + take])\n",
    "\n",
    "                start_index = (start_index + take) % len(questions)\n",
    "\n",
    "            yield self.vectorize(questions_batch, answers_batch)\n",
    "\n",
    "    def add_noise_to_string(self, a_string, amount_of_noise):\n",
    "        \"\"\"Add some artificial spelling mistakes to the string\"\"\"\n",
    "        if rand() < amount_of_noise * len(a_string):\n",
    "            # Replace a character with a random character\n",
    "            random_char_position = random_randint(len(a_string))\n",
    "            a_string = a_string[:random_char_position] + random_choice(CHARS[:-1]) + a_string[random_char_position + 1:]\n",
    "        if rand() < amount_of_noise * len(a_string):\n",
    "            # Delete a character\n",
    "            random_char_position = random_randint(len(a_string))\n",
    "            a_string = a_string[:random_char_position] + a_string[random_char_position + 1:]\n",
    "        if len(a_string) < MAX_INPUT_LEN and rand() < amount_of_noise * len(a_string):\n",
    "            # Add a random character\n",
    "            random_char_position = random_randint(len(a_string))\n",
    "            a_string = a_string[:random_char_position] + random_choice(CHARS[:-1]) + a_string[random_char_position:]\n",
    "        if rand() < amount_of_noise * len(a_string):\n",
    "            # Transpose 2 characters\n",
    "            random_char_position = random_randint(len(a_string) - 1)\n",
    "            a_string = (a_string[:random_char_position] +\n",
    "                        a_string[random_char_position + 1] +\n",
    "                        a_string[random_char_position] +\n",
    "                        a_string[random_char_position + 2:])\n",
    "        return a_string\n",
    "\n",
    "    def vectorize(self, questions, answers):\n",
    "        \"\"\"Vectorize the questions and expected answers\"\"\"\n",
    "\n",
    "        assert len(questions) == len(answers)\n",
    "\n",
    "        X = np_zeros((len(questions), self.x_max_length, self.character_table.size), dtype=np.bool)\n",
    "\n",
    "        for i in range(len(questions)):\n",
    "            sentence = questions[i]\n",
    "            for j, c in enumerate(sentence):\n",
    "                X[i, j, self.character_table.char_indices[c]] = 1\n",
    "\n",
    "        y = np_zeros((len(answers), self.y_max_length, self.character_table.size), dtype=np.bool)\n",
    "\n",
    "        for i in range(len(answers)):\n",
    "            sentence = answers[i]\n",
    "            for j, c in enumerate(sentence):\n",
    "                y[i, j, self.character_table.char_indices[c]] = 1\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Clean the text - remove unwanted chars, fold punctuation etc.\"\"\"\n",
    "\n",
    "        text = text.strip()\n",
    "        text = NORMALIZE_WHITESPACE_REGEX.sub(' ', text)\n",
    "        text = RE_DASH_FILTER.sub('-', text)\n",
    "        text = RE_APOSTROPHE_FILTER.sub(\"'\", text)\n",
    "        text = RE_LEFT_PARENTH_FILTER.sub(\"(\", text)\n",
    "        text = RE_RIGHT_PARENTH_FILTER.sub(\")\", text)\n",
    "        text = RE_BASIC_CLEANER.sub('', text)\n",
    "\n",
    "        return text\n",
    "\n",
    "    def read_news(self, dataset_filename):\n",
    "        \"\"\"Read the news corpus\"\"\"\n",
    "        print(\"Reading news\")\n",
    "        news = open(dataset_filename, encoding='utf-8',errors=\"ignore\").read()\n",
    "        print(\"Read news\")\n",
    "\n",
    "        lines = [line for line in news.split('\\n')]\n",
    "        #print(lines)#eliane\n",
    "        print(\"Read {} lines of input corpus\".format(len(lines)))\n",
    "\n",
    "        lines = [self.clean_text(line) for line in lines]\n",
    "        print(\"Cleaned text\")\n",
    "        #start eliane\n",
    "        myfile=open(\"testing/file_cleaned.txt\",\"w\",encoding=\"utf-8\",errors=\"ingore\")\n",
    "        for line in lines:\n",
    "            myfile.write(line+\"\\n\")\n",
    "        myfile.close()\n",
    "        #end eliane\n",
    "        counter = Counter()\n",
    "        for line in lines:\n",
    "            counter += Counter(line)\n",
    "        #print(counter.most_common(NUMBER_OF_CHARS))\n",
    "        most_popular_chars = {key for key, _value in counter.most_common(NUMBER_OF_CHARS)}\n",
    "        #print(most_popular_chars)\n",
    "        print(\"most popular characters are ready\")\n",
    "\n",
    "        lines = [line for line in lines if line and not bool(set(line) - most_popular_chars)]\n",
    "        print(\"Left with {} lines of input corpus\".format(len(lines)))\n",
    "\n",
    "        return lines\n",
    "\n",
    "    def generate_examples(self, corpus):\n",
    "        \"\"\"Generate examples of misspellings\"\"\"\n",
    "\n",
    "        print(\"Generating examples\")\n",
    "\n",
    "        questions, answers, seen_answers = [], [], set()\n",
    "\n",
    "        while corpus:\n",
    "            line = corpus.pop()\n",
    "            \n",
    "            while len(line) > MIN_INPUT_LEN:\n",
    "                if len(line) <= MAX_INPUT_LEN:\n",
    "                    answer = line\n",
    "                    line = \"\"\n",
    "                else:\n",
    "                    #print(line)\n",
    "                    \n",
    "                    space_location = line.rfind(\" \", MIN_INPUT_LEN, MAX_INPUT_LEN - 1)\n",
    "                    #print(space_location)\n",
    "                    if space_location > -1:\n",
    "                        answer = line[:space_location]\n",
    "                        line = line[len(answer) + 1:]\n",
    "                    else:\n",
    "                        space_location = line.rfind(\" \")  # no limits this time\n",
    "                        if space_location == -1:\n",
    "                            break  # we are done with this line\n",
    "                        else:\n",
    "                            line = line[space_location + 1:]\n",
    "                            continue\n",
    "\n",
    "                if answer and answer in seen_answers:\n",
    "                    continue\n",
    "\n",
    "                seen_answers.add(answer)\n",
    "                answers.append(answer)\n",
    "\n",
    "        print('Shuffle')\n",
    "        random_shuffle(answers)\n",
    "        print(\"Shuffled\")\n",
    "\n",
    "        for answer_index, answer in enumerate(answers):\n",
    "            question = self.add_noise_to_string(answer, AMOUNT_OF_NOISE)\n",
    "            question += '.' * (MAX_INPUT_LEN - len(question))\n",
    "            answer += \".\" * (MAX_INPUT_LEN - len(answer))\n",
    "            answers[answer_index] = answer\n",
    "            assert len(answer) == MAX_INPUT_LEN\n",
    "\n",
    "            question = question[::-1] if self.inverted else question\n",
    "            questions.append(question)\n",
    "\n",
    "        print(\"Generated questions and answers\")\n",
    "\n",
    "        return questions, answers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    \"\"\"\n",
    "    Given a set of characters:\n",
    "    + Encode them to a one hot integer representation\n",
    "    + Decode the one hot integer representation to their character output\n",
    "    + Decode a vector of probabilities to their character output\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, chars):\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "        self.size = len(self.chars)\n",
    "\n",
    "    def encode(self, C, maxlen):\n",
    "        \"\"\"Encode as one-hot\"\"\"\n",
    "        X = np_zeros((maxlen, len(self.chars)), dtype=np.bool)  # pylint:disable=no-member\n",
    "        for i, c in enumerate(C):\n",
    "            X[i, self.char_indices[c]] = 1\n",
    "        return X\n",
    "\n",
    "    def decode(self, X, calc_argmax=True):\n",
    "        \"\"\"Decode from one-hot\"\"\"\n",
    "        if calc_argmax:\n",
    "            X = X.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading news\n",
      "Read news\n",
      "Read 708 lines of input corpus\n",
      "Cleaned text\n",
      "most popular characters are ready\n",
      "Left with 623 lines of input corpus\n",
      "Generating examples\n",
      "Shuffle\n",
      "Shuffled\n",
      "Generated questions and answers\n",
      "Completed pre-processing\n",
      "Build model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\deptinfo\\Anaconda2\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:11: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(512, return_sequences=True, kernel_initializer=\"he_normal\")`\n",
      "  # This is added back by InteractiveShellApp.init_path()\n",
      "C:\\Users\\deptinfo\\Anaconda2\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:17: UserWarning: Update your `LSTM` call to the Keras 2 API: `LSTM(512, return_sequences=True, kernel_initializer=\"he_normal\")`\n",
      "C:\\Users\\deptinfo\\Anaconda2\\envs\\py36\\lib\\site-packages\\ipykernel_launcher.py:21: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(100, kernel_initializer=\"he_normal\")`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "20/20 [==============================] - 1536s 77s/step - loss: 3.1930 - acc: 0.2162 - val_loss: 2.8179 - val_acc: 0.2710\n",
      "Epoch 2/30\n",
      "20/20 [==============================] - 1452s 73s/step - loss: 2.8133 - acc: 0.2581 - val_loss: 2.7456 - val_acc: 0.2745\n",
      "Epoch 3/30\n",
      "20/20 [==============================] - 1465s 73s/step - loss: 2.7582 - acc: 0.2682 - val_loss: 2.7082 - val_acc: 0.2770\n",
      "Epoch 4/30\n",
      "20/20 [==============================] - 1473s 74s/step - loss: 2.7061 - acc: 0.2751 - val_loss: 2.6794 - val_acc: 0.2785\n",
      "Epoch 5/30\n",
      "20/20 [==============================] - 1483s 74s/step - loss: 2.6704 - acc: 0.2783 - val_loss: 2.6395 - val_acc: 0.2819\n",
      "Epoch 6/30\n",
      "20/20 [==============================] - 1490s 74s/step - loss: 2.6102 - acc: 0.2864 - val_loss: 2.5857 - val_acc: 0.2917\n",
      "Epoch 7/30\n",
      "20/20 [==============================] - 1496s 75s/step - loss: 2.5936 - acc: 0.2895 - val_loss: 2.5591 - val_acc: 0.2953\n",
      "Epoch 8/30\n",
      "20/20 [==============================] - 1499s 75s/step - loss: 2.5645 - acc: 0.2858 - val_loss: 2.5253 - val_acc: 0.2980\n",
      "Epoch 9/30\n",
      "20/20 [==============================] - 1500s 75s/step - loss: 2.5510 - acc: 0.2866 - val_loss: 2.4965 - val_acc: 0.3010\n",
      "Epoch 10/30\n",
      "20/20 [==============================] - 1499s 75s/step - loss: 2.4942 - acc: 0.2977 - val_loss: 2.4712 - val_acc: 0.3032\n",
      "Epoch 11/30\n",
      "20/20 [==============================] - 1502s 75s/step - loss: 2.4679 - acc: 0.2974 - val_loss: 2.4463 - val_acc: 0.3047\n",
      "Epoch 12/30\n",
      "20/20 [==============================] - 1506s 75s/step - loss: 2.4558 - acc: 0.2996 - val_loss: 2.4350 - val_acc: 0.3045\n",
      "Epoch 13/30\n",
      "20/20 [==============================] - 1505s 75s/step - loss: 2.4435 - acc: 0.3006 - val_loss: 2.4128 - val_acc: 0.3057\n",
      "Epoch 14/30\n",
      "20/20 [==============================] - 1514s 76s/step - loss: 2.4137 - acc: 0.3018 - val_loss: 2.3807 - val_acc: 0.3106\n",
      "Epoch 15/30\n",
      "19/20 [===========================>..] - ETA: 4s - loss: 2.4166 - acc: 0.2962"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-115-652ad43780c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m#main_news(args.checkpoint, args.datasetparams, args.initialepoch)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m     \u001b[0mmain_news\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m     \u001b[1;31m#dataset = DataSet(DATASET_FILENAME)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-112-2a3cf092a276>\u001b[0m in \u001b[0;36mmain_news\u001b[1;34m()\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcheckpoint_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m     \u001b[0miterate_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minitial_epoch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;31m#def read_news():\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-110-1c0d0d348ef9>\u001b[0m in \u001b[0;36miterate_training\u001b[1;34m(model, dataset, initial_epoch)\u001b[0m\n\u001b[0;32m     21\u001b[0m                         \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensorboard\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsv_logger\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_samples_callback\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                         \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m                         initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1424\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1425\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1426\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1427\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1428\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    209\u001b[0m                             \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m                             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 211\u001b[1;33m                             max_queue_size=max_queue_size)\n\u001b[0m\u001b[0;32m    212\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    213\u001b[0m                         \u001b[1;31m# No need for try/except because\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[0;32m   1478\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1479\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1480\u001b[1;33m             verbose=verbose)\n\u001b[0m\u001b[0;32m   1481\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1482\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(model, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[0;32m    323\u001b[0m                                  \u001b[1;34m'or (x, y). Found: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    324\u001b[0m                                  str(generator_output))\n\u001b[1;32m--> 325\u001b[1;33m             \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    326\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[1;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[0;32m   1259\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1260\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1261\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1262\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1263\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2659\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2660\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2661\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2662\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2663\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2630\u001b[0m                                 session)\n\u001b[1;32m-> 2631\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2632\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2633\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1449\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[1;32m-> 1451\u001b[1;33m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[0;32m   1452\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    #parser = argparse.ArgumentParser(description='Trains a deep spelling model.')\n",
    "    #parser.add_argument('--checkpoint', type=str,\n",
    "     #                   help='Filename of a model checkpoint to start the training from.')\n",
    "    #parser.add_argument('--datasetparams', type=str,\n",
    "     #                   help='Filename of a file with dataset params to load for continuing model training.')\n",
    "    #parser.add_argument('initialepoch', type=int,\n",
    "     #                   help='Initial epoch parameter for continuing model training.', default=0)\n",
    "\n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    #main_news(args.checkpoint, args.datasetparams, args.initialepoch)\n",
    "    main_news()\n",
    "    #dataset = DataSet(DATASET_FILENAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
